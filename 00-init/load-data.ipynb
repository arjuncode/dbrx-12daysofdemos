{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb3411b-fe03-4165-8c12-dfe6bebba611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.dbutils import DBUtils\n",
    "import os, re\n",
    "\n",
    "# ========================\n",
    "# CONFIG\n",
    "# ========================\n",
    "\n",
    "TARGET_CATALOG = \"main\"\n",
    "TARGET_SCHEMA  = \"dbrx_12daysofdemos\"\n",
    "\n",
    "CSV_FILES = [\n",
    "    \"holiday-sales-and-trends.csv\",\n",
    "    \"santa_letters_canada.csv\",\n",
    "]\n",
    "\n",
    "CSV_OPTIONS = {\n",
    "    \"header\": \"true\",\n",
    "    \"inferSchema\": \"true\",\n",
    "}\n",
    "\n",
    "# ========================\n",
    "# 1. Figure out this notebook's folder as a DBFS path\n",
    "# ========================\n",
    "\n",
    "dbutils = DBUtils(spark)\n",
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "notebook_path = ctx.notebookPath().get()          # e.g. /Workspace/Repos/user/repo/folder/notebook\n",
    "notebook_dir  = \"/\".join(notebook_path.split(\"/\")[:-1])\n",
    "\n",
    "# Use DBFS view of workspace files instead of \"file:\" (which hits WorkspaceLocalFileSystem)\n",
    "repo_dir_dbfs = f\"{notebook_dir}\"\n",
    "\n",
    "print(f\"Notebook path: {notebook_path}\")\n",
    "print(f\"Notebook dir:  {notebook_dir}\")\n",
    "print(f\"DBFS repo dir: {repo_dir_dbfs}\")\n",
    "\n",
    "# ========================\n",
    "# 2. USE catalog and ensure schema\n",
    "# ========================\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"USE CATALOG {TARGET_CATALOG}\")\n",
    "    print(f\"\\n✓ Using catalog: {TARGET_CATALOG}\")\n",
    "except AnalysisException as e:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not USE CATALOG {TARGET_CATALOG}. \"\n",
    "        f\"Make sure it exists and you have access.\"\n",
    "    ) from e\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA}\")\n",
    "print(f\"✓ Schema exists: {TARGET_CATALOG}.{TARGET_SCHEMA}\")\n",
    "\n",
    "# ========================\n",
    "# Helper: filename -> table name\n",
    "# ========================\n",
    "\n",
    "def clean_table_name(filename: str) -> str:\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    base = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", base).lower()\n",
    "    return base or \"table_from_csv\"\n",
    "\n",
    "# ========================\n",
    "# 3. Read each CSV from the repo folder and save as UC table\n",
    "# ========================\n",
    "\n",
    "for filename in CSV_FILES:\n",
    "    table_name      = clean_table_name(filename)\n",
    "    full_table_name = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{table_name}\"\n",
    "    csv_path        = f\"/Workspace{repo_dir_dbfs}/{filename}\"\n",
    "\n",
    "    print(f\"\\n=== Processing {filename} ===\")\n",
    "    print(f\"Source path:  {csv_path}\")\n",
    "    print(f\"Target table: {full_table_name}\")\n",
    "\n",
    "    # Read CSV -> DataFrame\n",
    "    try:\n",
    "        reader = spark.read\n",
    "        for k, v in CSV_OPTIONS.items():\n",
    "            reader = reader.option(k, v)\n",
    "\n",
    "        df: DataFrame = reader.csv(csv_path)\n",
    "        print(f\"DataFrame rows: {df.count()}\")\n",
    "\n",
    "        # Save as managed table in UC\n",
    "        df.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "        print(f\"✓ Created / replaced table {full_table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error for file at {csv_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nAll done creating tables from local repo CSVs.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load-data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
