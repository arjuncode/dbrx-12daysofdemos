{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8167771-2530-465b-bea0-dbccc1654372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Running init notebook with...\\n\\nTARGET_CATALOG={TARGET_CATALOG}\\n\\nTARGET_SCHEMA={TARGET_SCHEMA}\\n\\nTARGET_VOLUME={TARGET_VOLUME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb3411b-fe03-4165-8c12-dfe6bebba611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os, re\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "files_to_download = {\n",
    "    \"behavioral_analytics.csv\": (\n",
    "        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n",
    "        \"main/00-init/data/behavioral_analytics.csv\"\n",
    "    ),\n",
    "    \"delivery_optimization.csv\": (\n",
    "        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n",
    "        \"main/00-init/data/delivery_optimization.csv\"\n",
    "    ),\n",
    "    \"gift_requests.csv\": (\n",
    "        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n",
    "        \"main/00-init/data/gift_requests.csv\"\n",
    "    ),\n",
    "    \"holiday_sales_and_trends.csv\": (\n",
    "        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n",
    "        \"main/00-init/data/holiday_sales_and_trends.csv\"\n",
    "    ),\n",
    "    \"reindeer_telemetry.csv\": (\n",
    "        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n",
    "        \"main/00-init/data/reindeer_telemetry.csv\"\n",
    "    ),\n",
    "    \"santa_letters_canada.csv\": (\n",
    "        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n",
    "        \"main/00-init/data/santa_letters_canada.csv\"\n",
    "    ),\n",
    "    \"santa_letters_canada_with_emails.csv\": (\n",
    "        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n",
    "        \"main/00-init/data/santa_letters_canada_with_emails.csv\"\n",
    "    ),\n",
    "    \"workshop_production.csv\": (\n",
    "        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n",
    "        \"main/00-init/data/workshop_production.csv\"\n",
    "    )\n",
    "}\n",
    "\n",
    "CSV_OPTIONS = {\n",
    "    \"header\": \"true\",\n",
    "    \"inferSchema\": \"true\",\n",
    "    \"multiLine\": \"true\"\n",
    "}\n",
    "\n",
    "# ================================================================\n",
    "# 1. Use catalog and ensure schema + volume\n",
    "# ================================================================\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"USE CATALOG {TARGET_CATALOG}\")\n",
    "    print(f\"✓ Using catalog: {TARGET_CATALOG}\")\n",
    "except AnalysisException as e:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not USE CATALOG {TARGET_CATALOG}. Make sure it exists and you have access.\"\n",
    "    ) from e\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA}\")\n",
    "print(f\"✓ Schema exists: {TARGET_CATALOG}.{TARGET_SCHEMA}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"CREATE VOLUME IF NOT EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA}.{TARGET_VOLUME}\"\n",
    ")\n",
    "print(f\"✓ Volume exists: {TARGET_CATALOG}.{TARGET_SCHEMA}.{TARGET_VOLUME}\")\n",
    "\n",
    "volume_path = f\"/Volumes/{TARGET_CATALOG}/{TARGET_SCHEMA}/{TARGET_VOLUME}/\"\n",
    "print(f\"Volume path: {volume_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# 2. Download CSVs from GitHub → UC Volume\n",
    "# ================================================================\n",
    "\n",
    "for filename, url in files_to_download.items():\n",
    "    dest_path = volume_path + filename\n",
    "    print(f\"\\nDownloading {filename} from {url}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, dest_path)\n",
    "        print(f\"✓ Downloaded to {dest_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {filename}: {e}\")\n",
    "\n",
    "print(\"\\n=== Download complete. Listing files in volume ===\")\n",
    "try:\n",
    "    for f in dbutils.fs.ls(volume_path):\n",
    "        if f.name.endswith(\".csv\"):\n",
    "            print(f\"✓ {f.name} ({f.size} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing volume: {e}\")\n",
    "\n",
    "# ================================================================\n",
    "# 3. Create UC tables from the CSVs in the volume\n",
    "# ================================================================\n",
    "\n",
    "def clean_table_name(filename: str) -> str:\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    base = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", base).lower()\n",
    "    return base or \"table_from_csv\"\n",
    "\n",
    "for filename in files_to_download.keys():\n",
    "    table_name      = clean_table_name(filename)\n",
    "    full_table_name = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{table_name}\"\n",
    "    csv_path        = volume_path + filename\n",
    "\n",
    "    print(f\"\\n=== Loading {csv_path} into {full_table_name} ===\")\n",
    "\n",
    "    try:\n",
    "        reader = spark.read\n",
    "        for k, v in CSV_OPTIONS.items():\n",
    "            reader = reader.option(k, v)\n",
    "\n",
    "        df = reader.csv(csv_path)\n",
    "        print(f\"Rows read: {df.count()}\")\n",
    "\n",
    "        df.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "        print(f\"✓ Created / replaced table {full_table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating table {full_table_name} from {csv_path}: {e}\")\n",
    "\n",
    "print(\"\\nAll done! Tables are ready in Unity Catalog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4dcbe81-df20-490a-8986-2a591b7d00d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_source_table_schema(table_name):\n",
    "    print(f\"Ensuring schema columns exist for {table_name}...\")\n",
    "    try:\n",
    "        current_schema = spark.table(table_name).columns\n",
    "        columns_to_add = []\n",
    "\n",
    "        if \"en_route\" not in current_schema:\n",
    "            spark.sql(f\"ALTER TABLE {table_name} ADD COLUMN en_route BOOLEAN\")\n",
    "            spark.sql(f\"UPDATE {table_name} SET en_route = False\")\n",
    "\n",
    "        if \"delivered\" not in current_schema:\n",
    "            spark.sql(f\"ALTER TABLE {table_name} ADD COLUMN delivered BOOLEAN\")\n",
    "            spark.sql(f\"UPDATE {table_name} SET delivered = False\")\n",
    "\n",
    "        if \"cookies\" not in current_schema:\n",
    "            spark.sql(f\"ALTER TABLE {table_name} ADD COLUMN cookies INT\")\n",
    "            spark.sql(f\"UPDATE {table_name} SET cookies = Null\")\n",
    "\n",
    "        print(\"✓ Schema meets requirements.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error modifying source table. Error: {e}\")\n",
    "        raise e\n",
    "\n",
    "source_table_name = \"main.dbrx_12daysofdemos.gift_requests\"\n",
    "prepare_source_table_schema(source_table_name)\n",
    "\n",
    "spark.sql(f\"ALTER TABLE {source_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load-data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
