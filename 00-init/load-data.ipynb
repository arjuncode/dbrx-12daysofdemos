{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e257dc4b-843d-4d35-87bf-3bc4e7321c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "TARGET_CATALOG = \"main\"\n",
    "TARGET_SCHEMA  = \"dbrx_12daysofdemos\"\n",
    "TARGET_VOLUME  = \"raw_data_volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb3411b-fe03-4165-8c12-dfe6bebba611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "import urllib.request\nimport os, re\nfrom pyspark.sql.utils import AnalysisException\n\nfiles_to_download = {\n    \"behavioral_analytics.csv\": (\n        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n        \"main/00-init/data/behavioral_analytics.csv\"\n    ),\n    \"delivery_optimization.csv\": (\n        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n        \"main/00-init/data/delivery_optimization.csv\"\n    ),\n    \"gift_requests.csv\": (\n        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n        \"main/00-init/data/gift_requests.csv\"\n    ),\n    \"holiday_sales_and_trends.csv\": (\n        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n        \"main/00-init/data/holiday_sales_and_trends.csv\"\n    ),\n    \"reindeer_telemetry.csv\": (\n        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n        \"main/00-init/data/reindeer_telemetry.csv\"\n    ),\n    \"santa_letters_canada.csv\": (\n        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n        \"main/00-init/data/santa_letters_canada.csv\"\n    ),\n    \"santa_letters_canada_with_emails.csv\": (\n        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n        \"main/00-init/data/santa_letters_canada_with_emails.csv\"\n    ),\n    \"workshop_production.csv\": (\n        \"https://raw.githubusercontent.com/arjuncode/dbrx-12daysofdemos/\"\n        \"main/00-init/data/workshop_production.csv\"\n    )\n}\n\nCSV_OPTIONS = {\n    \"header\": \"true\",\n    \"inferSchema\": \"true\",\n    \"multiLine\": \"true\"\n}\n\n# ================================================================\n# 1. Use catalog and ensure schema + volume\n# ================================================================\n\ntry:\n    spark.sql(f\"USE CATALOG {TARGET_CATALOG}\")\n    print(f\"✓ Using catalog: {TARGET_CATALOG}\")\nexcept AnalysisException as e:\n    raise RuntimeError(\n        f\"Could not USE CATALOG {TARGET_CATALOG}. Make sure it exists and you have access.\"\n    ) from e\n\nspark.sql(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA}\")\nprint(f\"✓ Schema exists: {TARGET_CATALOG}.{TARGET_SCHEMA}\")\n\nspark.sql(\n    f\"CREATE VOLUME IF NOT EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA}.{TARGET_VOLUME}\"\n)\nprint(f\"✓ Volume exists: {TARGET_CATALOG}.{TARGET_SCHEMA}.{TARGET_VOLUME}\")\n\nvolume_path = f\"/Volumes/{TARGET_CATALOG}/{TARGET_SCHEMA}/{TARGET_VOLUME}/\"\nprint(f\"Volume path: {volume_path}\")\n\n# ================================================================\n# 2. Download CSVs from GitHub → UC Volume\n# ================================================================\n\nfor filename, url in files_to_download.items():\n    dest_path = volume_path + filename\n    print(f\"\\nDownloading {filename} from {url}\")\n    try:\n        urllib.request.urlretrieve(url, dest_path)\n        print(f\"✓ Downloaded to {dest_path}\")\n    except Exception as e:\n        print(f\"✗ Error downloading {filename}: {e}\")\n\nprint(\"\\n=== Download complete. Listing files in volume ===\")\ntry:\n    for f in dbutils.fs.ls(volume_path):\n        if f.name.endswith(\".csv\"):\n            print(f\"✓ {f.name} ({f.size} bytes)\")\nexcept Exception as e:\n    print(f\"Error listing volume: {e}\")\n\n# ================================================================\n# 3. Create UC tables from the CSVs in the volume\n# ================================================================\n\ndef clean_table_name(filename: str) -> str:\n    base = os.path.splitext(filename)[0]\n    base = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", base).lower()\n    return base or \"table_from_csv\"\n\nfor filename in files_to_download.keys():\n    table_name      = clean_table_name(filename)\n    full_table_name = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{table_name}\"\n    csv_path        = volume_path + filename\n\n    print(f\"\\n=== Loading {csv_path} into {full_table_name} ===\")\n\n    try:\n        reader = spark.read\n        for k, v in CSV_OPTIONS.items():\n            reader = reader.option(k, v)\n\n        df = reader.csv(csv_path)\n        print(f\"Rows read: {df.count()}\")\n\n        df.write.mode(\"overwrite\").saveAsTable(full_table_name)\n        print(f\"✓ Created / replaced table {full_table_name}\")\n    except Exception as e:\n        print(f\"✗ Error creating table {full_table_name} from {csv_path}: {e}\")\n\nprint(\"\\nAll done! Tables are ready in Unity Catalog.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4dcbe81-df20-490a-8986-2a591b7d00d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load-data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}