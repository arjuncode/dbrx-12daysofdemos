{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ac6fa7-e7b2-4f37-bfd0-663ca1d51100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéÖ What is Databricks Auto Loader?\n",
    "\n",
    "[Databricks Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) lets you scan a cloud storage folder (S3, ADLS, GCS) and only ingest the new data that arrived since the previous run.\n",
    "\n",
    "This is called **incremental ingestion**.\n",
    "\n",
    "Auto Loader can be used in a near real-time stream or in a batch fashion, e.g., running every night to ingest daily data.\n",
    "\n",
    "## üéÑ The North Pole's Challenge\n",
    "The North Pole has data stuck everywhere:\n",
    "* üì¨ **CSVs of children's letters** arriving from postal services worldwide\n",
    "* üè¨ **Retail store exports** tracking gift trends from mall Santa operations\n",
    "* ‚òÅÔ∏è **S3 buckets from regional elf teams** containing behavioral analytics, workshop IoT sensors, and reindeer telemetry\n",
    "\n",
    "## ‚ú® How Auto Loader Simplifies Data Ingestion\n",
    "\n",
    "Ingesting data at scale from cloud storage can be really hard. Auto Loader makes it easy, offering these benefits:\n",
    "\n",
    "* **Incremental** & **cost-efficient** ingestion (removes unnecessary listing or state handling)\n",
    "* **Simple** and **resilient** operation: no tuning or manual code required\n",
    "* Scalable to **billions of files**\n",
    "* **Schema inference** and **schema evolution** are handled out of the box for most formats (csv, json, avro, images...)\n",
    "\n",
    "### üéØ Mission: December 24th Deadline!\n",
    "This notebook demonstrates Auto Loader ingesting raw operational data from Unity Catalog volumes (simulating S3 buckets) into Delta tables.\n",
    "\n",
    "---\n",
    "*\"From scattered files to organized tables‚ÄîAuto Loader makes it magical!\"* ü¶å‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e2e7b0-6ab8-47cf-8580-6143f6eaa91f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üìÇ Explore the Volume (Simulated S3 Bucket)"
    }
   },
   "outputs": [],
   "source": [
    "# üéÑ Let's explore what data the regional elf teams have uploaded to our volume\n",
    "# This simulates an S3 bucket where CSV files are continuously arriving\n",
    "\n",
    "volume_path = \"/Volumes/12daysofdemos/raw_data/raw_data\"\n",
    "\n",
    "# List files in the volume\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "print(f\"‚ùÑÔ∏è Found {len(files)} files in the North Pole data volume:\")\n",
    "for file in files[:10]:  # Show first 10 files\n",
    "    print(f\"  - {file.name} ({file.size} bytes)\")\n",
    "\n",
    "# Preview a sample file\n",
    "print(\"\\nüîç Sample data from the first CSV file:\")\n",
    "display(spark.read.option(\"header\", \"true\").csv(volume_path).limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ab5987-0668-4f90-b06e-b5e6fc13326e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üéØ Schema Hints: Enforce Specific Types"
    }
   },
   "outputs": [],
   "source": [
    "# üéÖ AUTO LOADER: COMPLETE INGESTION PIPELINE\n",
    "# Single stream that reads from volume and writes to Delta table\n",
    "\n",
    "volume_source_path = \"/Volumes/12daysofdemos/raw_data/raw_data\"\n",
    "schema_location = \"/Volumes/12daysofdemos/raw_data/raw_data/_schemas/autoloader\"\n",
    "checkpoint_location = \"/Volumes/12daysofdemos/raw_data/raw_data/_checkpoints/autoloader\"\n",
    "target_table = \"12daysofdemos.raw_data.santas_workshop\"\n",
    "\n",
    "# Auto Loader stream: read from volume and write to Delta\n",
    "query = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(volume_source_path)\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .table(target_table))\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"‚úÖ Data successfully ingested into {target_table}!\")\n",
    "print(\"‚ú® Auto Loader handled schema inference and evolution automatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49127c07-5d67-4a67-878a-33ba3ca3dc47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üîç Verify Ingested Data"
    }
   },
   "outputs": [],
   "source": [
    "# üîç VERIFY THE INGESTED DATA\n",
    "# Let's check what was loaded into our Delta table\n",
    "\n",
    "target_table = \"12daysofdemos.raw_data.santas_workshop\"\n",
    "\n",
    "# Count total rows\n",
    "total_rows = spark.table(target_table).count()\n",
    "print(f\"üéÑ Total rows ingested: {total_rows:,}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüéÅ Sample data from santas_workshop table:\")\n",
    "display(spark.table(target_table).limit(10))\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nüìã Table schema (Auto Loader inferred this automatically):\")\n",
    "spark.table(target_table).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ec1c7e-9122-44e5-b80b-c05b223dfca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Schema Evolution with Auto Loader\n",
    "\n",
    "### How it works:\n",
    "1. **Initial ingestion**: Auto Loader samples files and infers the schema\n",
    "2. **New columns appear**: When new CSV files have additional columns, Auto Loader detects them\n",
    "3. **Automatic handling**: With `mergeSchema=true`, new columns are added to the Delta table\n",
    "4. **Rescued data**: If data doesn't match the schema, it's saved in `_rescued_data` column\n",
    "\n",
    "### üéØ Schema Evolution Modes:\n",
    "* **`addNewColumns`** (default): Add new columns, fail on type mismatches\n",
    "* **`rescue`**: Save incompatible data in `_rescued_data` column\n",
    "* **`failOnNewColumns`**: Fail the stream when new columns appear (requires manual restart)\n",
    "\n",
    "### ‚ú® Best Practices:\n",
    "* Use `cloudFiles.schemaLocation` to persist inferred schemas\n",
    "* Enable `mergeSchema=true` when writing to Delta for automatic schema evolution\n",
    "* Monitor `_rescued_data` column for data quality issues\n",
    "* Use schema hints for critical columns that need specific types\n",
    "\n",
    "---\n",
    "*Mrs. Claus approves: \"No more manual schema management!\"* üéÖ‚ú®"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
