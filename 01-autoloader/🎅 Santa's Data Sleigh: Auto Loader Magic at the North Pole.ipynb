{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ac6fa7-e7b2-4f37-bfd0-663ca1d51100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéÖ What is Databricks Auto Loader?\n",
    "\n",
    "[Databricks Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) lets you scan a cloud storage folder (S3, ADLS, GCS) and only ingest the new data that arrived since the previous run.\n",
    "\n",
    "This is called **incremental ingestion**.\n",
    "\n",
    "Auto Loader can be used in a near real-time stream or in a batch fashion, e.g., running every night to ingest daily data.\n",
    "\n",
    "## üéÑ The North Pole's Challenge\n",
    "The North Pole has data stuck everywhere:\n",
    "* üì¨ **CSVs of children's letters** arriving from postal services worldwide\n",
    "* üè¨ **Retail store exports** tracking gift trends from mall Santa operations\n",
    "* ‚òÅÔ∏è **S3 buckets from regional elf teams** containing behavioral analytics, workshop IoT sensors, and reindeer telemetry\n",
    "\n",
    "## ‚ú® How Auto Loader Simplifies Data Ingestion\n",
    "\n",
    "Ingesting data at scale from cloud storage can be really hard. Auto Loader makes it easy, offering these benefits:\n",
    "\n",
    "* **Incremental** & **cost-efficient** ingestion (removes unnecessary listing or state handling)\n",
    "* **Simple** and **resilient** operation: no tuning or manual code required\n",
    "* Scalable to **billions of files**\n",
    "* **Schema inference** and **schema evolution** are handled out of the box for most formats (csv, json, avro, images...)\n",
    "\n",
    "### üéØ Mission: December 24th Deadline!\n",
    "This notebook demonstrates Auto Loader ingesting raw operational data from Unity Catalog volumes (simulating S3 buckets) into Delta tables.\n",
    "\n",
    "---\n",
    "*\"From scattered files to organized tables‚ÄîAuto Loader makes it magical!\"* ü¶å‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49fb62b-5c19-427a-a320-2304b2e1cf1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üöÄ Start Streaming Reindeer Telemetry Data"
    }
   },
   "source": [
    "## ü¶å Start Streaming Reindeer Telemetry Data\n",
    "\n",
    "Before running the Auto Loader cells below, you need to start the streaming notebook that deposits files into the volume.\n",
    "\n",
    "### üìç Instructions:\n",
    "1. **Open the notebook**: `Stream_Reindeer_Telemetry_To_Volume` (in the same directory)\n",
    "2. **Run it in a separate tab**: This notebook will continuously deposit parquet files\n",
    "3. **Come back here**: Continue running the cells below to ingest the data with Auto Loader\n",
    "\n",
    "### üì° What it does:\n",
    "* Simulates real-time reindeer telemetry data arriving from sensors\n",
    "* Deposits parquet files into `/Volumes/12daysofdemos/raw_data/stream/reindeer_telemetry`\n",
    "* Runs continuously in the background\n",
    "\n",
    "---\n",
    "‚ú® **Tip**: Open `Stream_Reindeer_Telemetry_To_Volume` in a new browser tab so both notebooks can run simultaneously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c9069ea-5d55-4e64-ae3f-a2566a27f94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e2e7b0-6ab8-47cf-8580-6143f6eaa91f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üìÇ Explore the Volume (Simulated S3 Bucket)"
    }
   },
   "outputs": [],
   "source": [
    "# üéÑ Let's explore what data the regional elf teams have uploaded to our volume\n",
    "# This simulates an S3 bucket where Parquet files are continuously arriving\n",
    "\n",
    "volume_path = \"/Volumes/12daysofdemos/raw_data/stream/reindeer_telemetry\"\n",
    "\n",
    "# List files in the volume\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "print(f\"‚ùÑÔ∏è Found {len(files)} files in the North Pole data volume:\")\n",
    "for file in files[:10]:  # Show first 10 files\n",
    "    print(f\"  - {file.name} ({file.size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbfe4b98-214d-434f-ade7-bd58592c68ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d8487e-25f7-4ab3-83fa-c7359d2923a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üèóÔ∏è Create Bronze Schema"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the bronze_data schema if it doesn't exist\n",
    "-- This is where we'll deposit data after picking it up from raw_data volume\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS 12daysofdemos.bronze_data\n",
    "COMMENT 'Bronze layer: Raw ingested data from Auto Loader';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9864bd0d-4a4d-47f0-a587-0daa59620714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c625dcd-c28c-4d86-b916-9cb5340e0f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS 12daysofdemos.bronze_data.reindeer_telemetry;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ef18846-ec84-4e9e-9b42-4d00d996de91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ab5987-0668-4f90-b06e-b5e6fc13326e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üéØ Schema Hints: Enforce Specific Types"
    }
   },
   "outputs": [],
   "source": [
    "# üéÖ AUTO LOADER: COMPLETE INGESTION PIPELINE\n",
    "# Single stream that reads from volume and writes to Bronze Delta table\n",
    "\n",
    "volume_source_path = \"/Volumes/12daysofdemos/raw_data/stream/reindeer_telemetry\"\n",
    "# Checkpoint and schema locations are outside the source path to avoid reading metadata as data\n",
    "schema_location = \"/Volumes/12daysofdemos/raw_data/stream/_autoloader_schemas/reindeer_telemetry\"\n",
    "checkpoint_location = \"/Volumes/12daysofdemos/raw_data/stream/_autoloader_checkpoints/reindeer_telemetry\"\n",
    "target_table = \"12daysofdemos.bronze_data.reindeer_telemetry\"\n",
    "\n",
    "print(f\"‚è≥ Starting Auto Loader ingestion...\\n\")\n",
    "\n",
    "# Auto Loader stream: read from volume and write to Bronze Delta table\n",
    "query = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(volume_source_path)\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .table(target_table))\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"‚úÖ Data successfully ingested into {target_table}!\")\n",
    "print(\"‚ú® Auto Loader handled schema inference and evolution automatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba54e19d-1bf9-439e-8f23-f5f97a897d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ec5d9b-4002-4a6f-8b0a-6586339bfeb1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "‚úÖ Verify Fixed Table"
    }
   },
   "outputs": [],
   "source": [
    "# üîç VERIFY THE INGESTED DATA\n",
    "# Let's check what was loaded into our Bronze Delta table\n",
    "\n",
    "target_table = \"12daysofdemos.bronze_data.reindeer_telemetry\"\n",
    "\n",
    "# Count total rows\n",
    "total_rows = spark.table(target_table).count()\n",
    "print(f\"üéÑ Total rows ingested: {total_rows:,}\")\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nüìã Table schema (Auto Loader inferred this automatically):\")\n",
    "spark.table(target_table).printSchema()\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüéÅ Sample data from bronze reindeer_telemetry table:\")\n",
    "display(spark.table(target_table).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ec1c7e-9122-44e5-b80b-c05b223dfca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Schema Evolution with Auto Loader\n",
    "\n",
    "### How it works:\n",
    "1. **Initial ingestion**: Auto Loader samples files and infers the schema\n",
    "2. **New columns appear**: When new CSV files have additional columns, Auto Loader detects them\n",
    "3. **Automatic handling**: With `mergeSchema=true`, new columns are added to the Delta table\n",
    "4. **Rescued data**: If data doesn't match the schema, it's saved in `_rescued_data` column\n",
    "\n",
    "### üéØ Schema Evolution Modes:\n",
    "* **`addNewColumns`** (default): Add new columns, fail on type mismatches\n",
    "* **`rescue`**: Save incompatible data in `_rescued_data` column\n",
    "* **`failOnNewColumns`**: Fail the stream when new columns appear (requires manual restart)\n",
    "\n",
    "### ‚ú® Best Practices:\n",
    "* Use `cloudFiles.schemaLocation` to persist inferred schemas\n",
    "* Enable `mergeSchema=true` when writing to Delta for automatic schema evolution\n",
    "* Monitor `_rescued_data` column for data quality issues\n",
    "* Use schema hints for critical columns that need specific types\n",
    "\n",
    "---\n",
    "*Mrs. Claus approves: \"No more manual schema management!\"* üéÖ‚ú®"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8018969931878769,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "üéÖ Santa's Data Sleigh: Auto Loader Magic at the North Pole",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
