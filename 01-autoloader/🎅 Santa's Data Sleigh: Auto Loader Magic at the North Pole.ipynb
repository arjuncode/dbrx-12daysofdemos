{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ac6fa7-e7b2-4f37-bfd0-663ca1d51100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéÖ What is Databricks Auto Loader?\n",
    "\n",
    "[Databricks Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) lets you scan a cloud storage folder (S3, ADLS, GCS) and only ingest the new data that arrived since the previous run.\n",
    "\n",
    "This is called **incremental ingestion**.\n",
    "\n",
    "Auto Loader can be used in a near real-time stream or in a batch fashion, e.g., running every night to ingest daily data.\n",
    "\n",
    "## üéÑ The North Pole's Challenge\n",
    "The North Pole has data stuck everywhere:\n",
    "* üì¨ **CSVs of children's letters** arriving from postal services worldwide\n",
    "* üè¨ **Retail store exports** tracking gift trends from mall Santa operations\n",
    "* ‚òÅÔ∏è **S3 buckets from regional elf teams** containing behavioral analytics, workshop IoT sensors, and reindeer telemetry\n",
    "\n",
    "## ‚ú® How Auto Loader Simplifies Data Ingestion\n",
    "\n",
    "Ingesting data at scale from cloud storage can be really hard. Auto Loader makes it easy, offering these benefits:\n",
    "\n",
    "* **Incremental** & **cost-efficient** ingestion (removes unnecessary listing or state handling)\n",
    "* **Simple** and **resilient** operation: no tuning or manual code required\n",
    "* Scalable to **billions of files**\n",
    "* **Schema inference** and **schema evolution** are handled out of the box for most formats (csv, json, avro, images...)\n",
    "\n",
    "### üéØ Mission: December 24th Deadline!\n",
    "This notebook demonstrates Auto Loader ingesting raw operational data from Unity Catalog volumes (simulating S3 buckets) into Delta tables.\n",
    "\n",
    "---\n",
    "*\"From scattered files to organized tables‚ÄîAuto Loader makes it magical!\"* ü¶å‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49fb62b-5c19-427a-a320-2304b2e1cf1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üöÄ Start Streaming Reindeer Telemetry Data"
    }
   },
   "source": [
    "## ü¶å Start Streaming Reindeer Telemetry Data\n",
    "\n",
    "Before running the Auto Loader cells below, you need to start the streaming notebook that deposits files into the volume.\n",
    "\n",
    "### üìç Instructions:\n",
    "1. **Open the notebook**: `Stream_Reindeer_Telemetry_To_Volume` (in the same directory)\n",
    "2. **Run it in a separate tab**: This notebook will continuously deposit parquet files\n",
    "3. **Come back here**: Continue running the cells below to ingest the data with Auto Loader\n",
    "\n",
    "### üì° What it does:\n",
    "* Simulates real-time reindeer telemetry data arriving from sensors\n",
    "* Deposits parquet files into `/Volumes/.../stream/reindeer_telemetry`\n",
    "* Runs continuously in the background\n",
    "\n",
    "---\n",
    "‚ú® **Tip**: Open `Stream_Reindeer_Telemetry_To_Volume` in a new browser tab so both notebooks can run simultaneously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31a8ea1-d72c-4eae-8b8c-53a8b6a5217e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "‚öôÔ∏è Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# TODO: Update these values for your environment\n",
    "TARGET_CATALOG = \"main\"\n",
    "TARGET_SCHEMA = \"dbrx_12daysofdemos\"\n",
    "TARGET_BRONZE_SCHEMA = \"bronze_data\"\n",
    "TARGET_VOLUME = \"raw_data_volume\"\n",
    "\n",
    "# Derived paths - no need to modify these\n",
    "volume_base_path = f\"/Volumes/{TARGET_CATALOG}/{TARGET_SCHEMA}/stream\"\n",
    "volume_source_path = f\"{volume_base_path}/reindeer_telemetry\"\n",
    "schema_location = f\"{volume_base_path}/_autoloader_schemas/reindeer_telemetry\"\n",
    "checkpoint_location = f\"{volume_base_path}/_autoloader_checkpoints/reindeer_telemetry\"\n",
    "target_table = f\"{TARGET_CATALOG}.{TARGET_BRONZE_SCHEMA}.reindeer_telemetry\"\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded:\")\n",
    "print(f\"   üìÅ Source: {volume_source_path}\")\n",
    "print(f\"   üéØ Target: {target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f9102c-dafe-4957-9c22-ce9664ddc343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../00-init/load-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e2e7b0-6ab8-47cf-8580-6143f6eaa91f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üìÇ Explore the Volume (Simulated S3 Bucket)"
    }
   },
   "outputs": [],
   "source": [
    "# üéÑ Let's explore what data the regional elf teams have uploaded to our volume\n",
    "# This simulates an S3 bucket where Parquet files are continuously arriving\n",
    "\n",
    "# List files in the volume\n",
    "files = dbutils.fs.ls(volume_source_path)\n",
    "print(f\"‚ùÑÔ∏è Found {len(files)} files in the North Pole data volume:\")\n",
    "for file in files[:10]:  # Show first 10 files\n",
    "    print(f\"  - {file.name} ({file.size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d8487e-25f7-4ab3-83fa-c7359d2923a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üèóÔ∏è Create Bronze Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Create the bronze_data schema if it doesn't exist\n",
    "# This is where we'll deposit data after picking it up from raw_data volume\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {TARGET_CATALOG}.{TARGET_BRONZE_SCHEMA}\n",
    "    COMMENT 'Bronze layer: Raw ingested data from Auto Loader'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Schema {TARGET_CATALOG}.{TARGET_BRONZE_SCHEMA} is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c625dcd-c28c-4d86-b916-9cb5340e0f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional: Drop the table if you want to start fresh\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "print(f\"üßπ Cleaned up {target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ab5987-0668-4f90-b06e-b5e6fc13326e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üéØ Schema Hints: Enforce Specific Types"
    }
   },
   "outputs": [],
   "source": [
    "# üéÖ AUTO LOADER: COMPLETE INGESTION PIPELINE\n",
    "# Single stream that reads from volume and writes to Bronze Delta table\n",
    "\n",
    "print(f\"‚è≥ Starting Auto Loader ingestion...\\n\")\n",
    "\n",
    "# Auto Loader stream: read from volume and write to Bronze Delta table\n",
    "query = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(volume_source_path)\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)  # Trigger once when data is available\n",
    "    .table(target_table))\n",
    "\n",
    "print(f\"‚úÖ Stream started! Continuously monitoring {volume_source_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ec5d9b-4002-4a6f-8b0a-6586339bfeb1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "‚úÖ Verify Fixed Table"
    }
   },
   "outputs": [],
   "source": [
    "# üîç VERIFY THE INGESTED DATA\n",
    "# Let's check what was loaded into our Bronze Delta table\n",
    "from time import sleep\n",
    "print('Wait for the stream to process some records..')\n",
    "sleep(15)\n",
    "\n",
    "# Count total rows\n",
    "total_rows = spark.table(target_table).count()\n",
    "print(f\"üéÑ Total rows ingested: {total_rows:,}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nüéÅ Sample data from {target_table}:\")\n",
    "display(spark.table(target_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ec1c7e-9122-44e5-b80b-c05b223dfca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Schema Evolution with Auto Loader\n",
    "\n",
    "### How it works:\n",
    "1. **Initial ingestion**: Auto Loader samples files and infers the schema\n",
    "2. **New columns appear**: When new CSV files have additional columns, Auto Loader detects them\n",
    "3. **Automatic handling**: With `mergeSchema=true`, new columns are added to the Delta table\n",
    "4. **Rescued data**: If data doesn't match the schema, it's saved in `_rescued_data` column\n",
    "\n",
    "### üéØ Schema Evolution Modes:\n",
    "* **`addNewColumns`** (default): Add new columns, fail on type mismatches\n",
    "* **`rescue`**: Save incompatible data in `_rescued_data` column\n",
    "* **`failOnNewColumns`**: Fail the stream when new columns appear (requires manual restart)\n",
    "\n",
    "### ‚ú® Best Practices:\n",
    "* Use `cloudFiles.schemaLocation` to persist inferred schemas\n",
    "* Enable `mergeSchema=true` when writing to Delta for automatic schema evolution\n",
    "* Monitor `_rescued_data` column for data quality issues\n",
    "* Use schema hints for critical columns that need specific types\n",
    "\n",
    "---\n",
    "*Mrs. Claus approves: \"No more manual schema management!\"* üéÖ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40eb9e1-957a-4e31-adb1-1730bc0148a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üõë Stop All Streams"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "print(\"üö® If you want to re-run cells, you can cancel this cell! All this is doing is turning off your streams! Please make sure to turn off your streams before you leave using stream.stop()! üö®\")\n",
    "\n",
    "# Stop all active streams to prevent them from continuing to run\n",
    "sleep(180)\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"‚èπÔ∏è Stopping stream: {stream.name if stream.name else stream.id}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"\\n‚úÖ All streams stopped successfully!\")\n",
    "print(\"üéÑ Auto Loader ingestion complete. Happy holidays!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "üéÖ Santa's Data Sleigh: Auto Loader Magic at the North Pole",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
