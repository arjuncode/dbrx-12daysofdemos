{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50e9894-6a12-4b8c-b898-bc54115f90c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "TARGET_CATALOG = \"main\"\n",
    "TARGET_SCHEMA  = \"dbrx_12daysofdemos\"\n",
    "TARGET_VOLUME  = \"raw_data_volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93533208-e9dc-43cb-ae1a-5b4220f85a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "print(f\"Catalog: {TARGET_CATALOG}\")\n",
    "print(f\"Schema:  {TARGET_SCHEMA}\")\n",
    "print(f\"Volume:  {TARGET_VOLUME}\")\n",
    "\n",
    "# ================================================================\n",
    "# 1. Use catalog\n",
    "# ================================================================\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"USE CATALOG {TARGET_CATALOG}\")\n",
    "    print(f\"✓ Using catalog: {TARGET_CATALOG}\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"⚠️ Could not USE CATALOG {TARGET_CATALOG}: {e}\")\n",
    "    raise\n",
    "\n",
    "# ================================================================\n",
    "# 2. Drop volume if exists\n",
    "# ================================================================\n",
    "\n",
    "volume_fqn = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{TARGET_VOLUME}\"\n",
    "\n",
    "print(f\"\\nAttempting to drop volume: {volume_fqn}\")\n",
    "try:\n",
    "    spark.sql(f\"DROP VOLUME IF EXISTS {volume_fqn}\")\n",
    "    print(f\"✓ Dropped volume (if it existed): {volume_fqn}\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"⚠️ Could not drop volume {volume_fqn}: {e}\")\n",
    "\n",
    "# ================================================================\n",
    "# 3. Drop schema (CASCADE)\n",
    "# ================================================================\n",
    "\n",
    "schema_fqn = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}\"\n",
    "\n",
    "print(f\"\\nAttempting to drop schema (CASCADE): {schema_fqn}\")\n",
    "try:\n",
    "    spark.sql(f\"DROP SCHEMA IF EXISTS {schema_fqn} CASCADE\")\n",
    "    print(f\"✓ Dropped schema (if it existed): {schema_fqn}\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"⚠️ Could not drop schema {schema_fqn}: {e}\")\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cleanup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
