{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ac6fa7-e7b2-4f37-bfd0-663ca1d51100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**12 Days of Demos**\n",
    "# üéÖ Auto Loader Magic at the North Pole üéÑ\n",
    "\n",
    "[Databricks Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) lets elf data teams scan a cloud storage folder (S3, ADLS, GCS) and only ingest the new data that arrived since the previous run. This notebook demonstrates Auto Loader ingesting raw operational data from Unity Catalog volumes (simulating S3 buckets) into Delta tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd72543e-c938-458c-8606-c2c6fd1dcee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e49fb62b-5c19-427a-a320-2304b2e1cf1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "üöÄ Start Streaming Reindeer Telemetry Data"
    }
   },
   "source": [
    "### ü¶å Step 1: Configuration\n",
    "\n",
    "The configuration settings below are where the demo will load and create data. You may choose to optionally change the settings if you prefer the demo use a different catalog or schema.\n",
    "\n",
    "üëá **Optionally update the cell below, then run it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31a8ea1-d72c-4eae-8b8c-53a8b6a5217e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "‚öôÔ∏è Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Optionally update these values for your environment\n",
    "TARGET_CATALOG = \"main\"\n",
    "TARGET_SCHEMA = \"dbrx_12daysofdemos\"\n",
    "TARGET_BRONZE_SCHEMA = \"bronze_data\"\n",
    "TARGET_VOLUME = \"raw_data_volume\"\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df53dd67-287e-4f34-aab9-04cedfe42148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Derived paths - do not modify these\n",
    "volume_base_path = f\"/Volumes/{TARGET_CATALOG}/{TARGET_SCHEMA}/stream\"\n",
    "volume_source_path = f\"{volume_base_path}/reindeer_telemetry\"\n",
    "schema_location = f\"{volume_base_path}/_autoloader_schemas/reindeer_telemetry\"\n",
    "checkpoint_location = f\"{volume_base_path}/_autoloader_checkpoints/reindeer_telemetry\"\n",
    "target_table = f\"{TARGET_CATALOG}.{TARGET_BRONZE_SCHEMA}.reindeer_telemetry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f9102c-dafe-4957-9c22-ce9664ddc343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../00-init/load-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afa0aef2-61f5-4473-8f42-b702ed76ba76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before running the Auto Loader cells below, you need to start the streaming notebook that deposits files into the volume.\n",
    "\n",
    "üìç **Instructions:**\n",
    "1. **Open the notebook**: `Stream_Reindeer_Telemetry_To_Volume` (in the same directory)\n",
    "2. **Run it in a separate tab**: This notebook will continuously deposit parquet files\n",
    "3. **Come back here**: Continue running the cells below to ingest the data with Auto Loader\n",
    "\n",
    "üì° **What it does:**\n",
    "* Simulates real-time reindeer telemetry data arriving from sensors\n",
    "* Deposits parquet files into `/Volumes/.../stream/reindeer_telemetry`\n",
    "* Runs continuously in the background\n",
    "\n",
    "---\n",
    "‚ú® **Tip**: Open `Stream_Reindeer_Telemetry_To_Volume` in a new browser tab so both notebooks can run simultaneously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e575d643-c518-4fce-9d3e-a078eaeece92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç Step 2: Explore Data Uploaded to Volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e2e7b0-6ab8-47cf-8580-6143f6eaa91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# üéÑ Let's explore what data the regional elf teams have uploaded to our volume\n",
    "# This simulates an S3 bucket where Parquet files are continuously arriving\n",
    "\n",
    "# List files in the volume\n",
    "files = dbutils.fs.ls(volume_source_path)\n",
    "print(f\"‚ùÑÔ∏è Found {len(files)} files in the North Pole data volume:\")\n",
    "for file in files[:10]:  # Show first 10 files\n",
    "    print(f\"  - {file.name} ({file.size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a64c176-8efd-48cd-bb2d-215e3a98b569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ü•â Step 3: Create Bronze Data Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d8487e-25f7-4ab3-83fa-c7359d2923a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is where we'll deposit data after picking it up from raw_data volume\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {TARGET_CATALOG}.{TARGET_BRONZE_SCHEMA}\n",
    "    COMMENT 'Bronze layer: Raw ingested data from Auto Loader'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Schema {TARGET_CATALOG}.{TARGET_BRONZE_SCHEMA} is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c625dcd-c28c-4d86-b916-9cb5340e0f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional: Drop the table if you want to start fresh\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "print(f\"üßπ Cleaned up {target_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68391072-47a4-4e05-8036-59ca60b98159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ü§ñ Step 4: Setup Auto Loader to Read from Volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ab5987-0668-4f90-b06e-b5e6fc13326e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Single stream that reads from volume and writes to Bronze Delta table\n",
    "\n",
    "print(f\"‚è≥ Starting Auto Loader ingestion...\\n\")\n",
    "\n",
    "# Auto Loader stream: read from volume and write to Bronze Delta table\n",
    "query = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(volume_source_path)\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)  # Trigger once when data is available\n",
    "    .table(target_table))\n",
    "\n",
    "print(f\"‚úÖ Stream started! Continuously monitoring {volume_source_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a25da58-f3e2-44be-8746-a318d175f07a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ  Step 4: Verify Fixed Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ec5d9b-4002-4a6f-8b0a-6586339bfeb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's check what was loaded into our Bronze Delta table\n",
    "from time import sleep\n",
    "print('Wait for the stream to process some records..')\n",
    "sleep(15)\n",
    "\n",
    "# Count total rows\n",
    "total_rows = spark.table(target_table).count()\n",
    "print(f\"üéÑ Total rows ingested: {total_rows:,}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nüéÅ Sample data from {target_table}:\")\n",
    "display(spark.table(target_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ec1c7e-9122-44e5-b80b-c05b223dfca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìö Schema Evolution with Auto Loader\n",
    "\n",
    "**How it works:**\n",
    "1. **Initial ingestion**: Auto Loader samples files and infers the schema\n",
    "2. **New columns appear**: When new CSV files have additional columns, Auto Loader detects them\n",
    "3. **Automatic handling**: With `mergeSchema=true`, new columns are added to the Delta table\n",
    "4. **Rescued data**: If data doesn't match the schema, it's saved in `_rescued_data` column\n",
    "\n",
    "**üéØ Schema Evolution Modes:**\n",
    "* **`addNewColumns`** (default): Add new columns, fail on type mismatches\n",
    "* **`rescue`**: Save incompatible data in `_rescued_data` column\n",
    "* **`failOnNewColumns`**: Fail the stream when new columns appear (requires manual restart)\n",
    "\n",
    "**‚ú® Best Practices:**\n",
    "* Use `cloudFiles.schemaLocation` to persist inferred schemas\n",
    "* Enable `mergeSchema=true` when writing to Delta for automatic schema evolution\n",
    "* Monitor `_rescued_data` column for data quality issues\n",
    "* Use schema hints for critical columns that need specific types\n",
    "\n",
    "---\n",
    "*Mrs. Claus approves: \"No more manual schema management!\"* üéÖ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "657d2620-1d98-4e71-b439-c19e251e1978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üõë Stop All Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40eb9e1-957a-4e31-adb1-1730bc0148a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "print(\"üö® If you want to re-run cells, you can cancel this cell! All this is doing is turning off your streams! Please make sure to turn off your streams before you leave using stream.stop()! üö®\")\n",
    "\n",
    "# Stop all active streams to prevent them from continuing to run\n",
    "sleep(180)\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"‚èπÔ∏è Stopping stream: {stream.name if stream.name else stream.id}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"\\n‚úÖ All streams stopped successfully!\")\n",
    "print(\"üéÑ Auto Loader ingestion complete. Happy holidays!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "auto-loader-demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
