{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a1c5ab",
   "metadata": {},
   "source": [
    "# üéÑ 12 Days of Demos: North Pole Modernization Office (NPMO)\n",
    "\n",
    "Welcome to the **North Pole Modernization Office**! As part of our initiative to move from handwritten letters and spreadsheets to an AI-driven Lakehouse, we need to ingest our legacy data.\n",
    "\n",
    "This notebook initializes the environment for the **Model Context Protocol (MCP)** demo by:\n",
    "1. Creating and or setting the Unity Catalog and Schema.\n",
    "2. Ingesting synthetic CSV data (Gift Requests, Reindeer Telemetry, etc.) into Delta Tables.\n",
    "\n",
    "*Let's get this data ready before Christmas Eve!* üéÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81262a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Configuration\n",
    "# TODO: Update these values for your environment\n",
    "catalog_name = \"12daysofdemos\"\n",
    "schema_name = \"npmo\"\n",
    "\n",
    "# Path where the CSV files are located in the repo\n",
    "source_data_path = Path(f\"{os.getcwd()}/data\")\n",
    "volume_data_path = f\"/Volumes/{catalog_name}/{schema_name}/data\"\n",
    "print(source_data_path)\n",
    "print(volume_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Catalog and Schema\n",
    "# spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "# spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS data\")\n",
    "\n",
    "print(f\"Using Catalog: {catalog_name}, Schema: {schema_name}, Volume: data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34b703",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Ingest Holiday Themed Data to Delta\n",
    "We are taking our raw CSV files‚Äîrepresenting everything from *Reindeer Telemetry* to *Gift Requests*‚Äîand loading them into managed Delta tables. This provides the foundation for our AI agents to query and analyze North Pole operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_to_delta(table_name, file_name):\n",
    "    # First copy file to a Volume so Spark can get it\n",
    "    source_file_path = f\"{source_data_path}/{file_name}\"\n",
    "    volume_file_path = f\"{volume_data_path}/{file_name}\"\n",
    "    shutil.copy(source_file_path, volume_file_path)\n",
    "\n",
    "    # Start ingestion\n",
    "    print(f\"Ingesting {volume_file_path} into table {table_name}...\")\n",
    "    try:\n",
    "        # Read CSV with header and infer schema\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(volume_file_path)\n",
    "            \n",
    "        # Write to Delta table\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(table_name)\n",
    "            \n",
    "        print(f\"‚úÖ Successfully created table: {catalog_name}.{schema_name}.{table_name}\")\n",
    "        print(f\"   Row count: {df.count()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error ingesting {table_name}: {str(e)}\")\n",
    "\n",
    "# List of datasets to ingest\n",
    "datasets = [\n",
    "    (\"gift_requests\", \"gift_requests.csv\"),\n",
    "    (\"reindeer_telemetry\", \"reindeer_telemetry.csv\"),\n",
    "    (\"workshop_production\", \"workshop_production.csv\"),\n",
    "    (\"behavioral_analytics\", \"behavioral_analytics.csv\"),\n",
    "    (\"delivery_optimization\", \"delivery_optimization.csv\")\n",
    "]\n",
    "\n",
    "# Run ingestion\n",
    "for table, file in datasets:\n",
    "    ingest_csv_to_delta(table, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763187d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tables\n",
    "display(spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema_name}\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
